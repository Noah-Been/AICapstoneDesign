# 증권사 리포트 수집 및 처리 파이프라인 구축 (2025-10-20)

## 주요 목표
증권사 웹사이트에서 리서치 리포트(PDF)를 자동으로 수집하고, 내용을 추출 및 정제하여 분석 가능한 데이터로 가공하는 자동화 파이프라인을 구축한다.

## 주요 성과 및 완료된 작업

### 1. 미래에셋증권 리포트 처리 자동화 파이프라인 완성
- **`run_mirae_pipeline.py`** 마스터 스크립트를 통해 PDF 다운로드, OCR, Gemini API 텍스트 정제, 최종 데이터 저장까지의 모든 과정을 자동화했습니다.

### 2. 한화투자증권 리포트 처리 파이프라인 구축 (웹 크롤링 기반)
- **`run_hanwha_pipeline.py`** 스크립트를 수정하여 PDF 다운로드 대신, 로그인 없이 접근 가능한 웹 페이지의 요약 본문을 직접 크롤링하여 `content`를 생성하도록 구현했습니다.

### 3. 유진투자증권 리포트 처리 파이프라인 완성 (PDF/OCR 기반)
- **`run_eugene_pipeline.py`** 스크립트를 새로 생성하여 PDF 기반의 리포트 처리 자동화를 완성했습니다.
- 개발 과정에서 리포트가 "기업분석"과 "산업분석" 두 페이지에 나뉘어 다른 구조로 올라오는 것을 발견, 두 가지 HTML 구조를 모두 파싱하도록 로직을 개선하여 안정성을 높였습니다.

### 4. 삼성증권 리포트 처리 파이프라인 완성 (PDF/OCR 기반)
- **`run_samsung_pipeline.py`** 스크립트를 새로 생성하여 삼성증권 리포트 처리 자동화를 완성했습니다.
- **주요 특징:**
    - 초기 분석 시 로그인 알림(alert) 창으로 인해 접근이 어려워 보였으나, 재분석 결과 해당 알림과 무관하게 원본 HTML에 PDF 링크가 직접 포함된 것을 확인했습니다.
    - 이 분석을 바탕으로, 페이지의 `table` 구조를 직접 파싱하여 PDF 링크를 수집하고, 다른 증권사와 동일하게 **PDF 다운로드 → OCR → Gemini 정제** 파이프라인을 적용했습니다.
- **결과:**
    - 이제 `run_samsung_pipeline.py`를 실행하면 해당 날짜의 삼성증권 리포트가 `all.jsonl` 및 `by_house/samsung.jsonl`에 자동으로 추가됩니다.

### 5. Gemini API 사용량 제한 문제 해결 및 모델 최적화
- 다수의 리포트를 한 번에 처리할 때 발생하던 Gemini API의 분당 요청 횟수(RPM) 제한 문제를 해결했습니다.
- **해결 방안:**
    - **모델 변경:** 텍스트 정제에 사용되는 모델을 기존 `gemini-pro-latest`에서 RPM이 더 높고 빠른 **`gemini-2.0-flash-lite`**로 변경했습니다.
    - **시간 지연 추가:** 각 리포트 처리 사이에 3초의 대기 시간을 추가하여 API 서버의 부담을 줄였습니다.
- **적용 범위:** 이 최적화는 PDF를 처리하는 모든 파이프라인(`run_mirae_pipeline.py`, `run_eugene_pipeline.py`, `run_samsung_pipeline.py`)에 적용하여 향후 안정적인 실행을 보장합니다.

### 6. 안전한 API 키 관리 시스템 구축
- 프로젝트의 모든 API 키(Gemini, LS증권, 네이버)를 코드에서 분리하여 **`.env` 파일**에서 통합 관리하도록 시스템을 구축했습니다.
- `python-dotenv` 라이브러리를 사용하여 모든 관련 스크립트가 실행 시 자동으로 `.env` 파일의 키를 읽어오도록 수정했습니다.

### 7. 시스템 환경 구축
- OCR 실행에 필요한 `Tesseract`와 `poppler-utils` 패키지를 시스템에 설치하고 정상 동작을 확인했습니다.
- Gemini API 사용에 필요한 `google-generativeai` 파이썬 라이브러리를 가상 환경에 설치했습니다.

## 향후 계획
- 현재 구축된 4사(미래에셋, 한화, 유진, 삼성)의 파이프라인을 모델로 삼아, 나머지 증권사의 크롤러도 동일한 방식으로 구현합니다.
- 최종적으로 정제된 `jsonl` 데이터를 RAG 챗봇의 지식 베이스로 활용하기 위한 다음 단계의 데이터 처리 및 임베딩 작업을 준비합니다.